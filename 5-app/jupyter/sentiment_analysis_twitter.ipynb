{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,tweepy,csv,re\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import nltk\n",
    "import Sastrawi\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, StopWordRemover, ArrayDictionary\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysis:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tweets = []\n",
    "        self.tweetText = []\n",
    "    def DownloadData(self,searchTerm, NoOfTerms):\n",
    "        # authenticating\n",
    "        print(searchTerm, NoOfTerms)\n",
    "        consumerKey='qubRP6d5B5eCD3RHrgoMRkDjH'\n",
    "        consumerSecret='AJlk1MiHfXqnhByWtCf1Mbi2T9WGW08hoLkkDIbks4ztUpFx99'\n",
    "        accessToken='3180202308-DmrgiSoxUUoSRevLLPldgCE57M0eD117K0mhFG5'\n",
    "        accessTokenSecret='fuY5Hkk838OnmGOZUb16KLkSDw3JZjflxXQ1NHP6pP9Jr'\n",
    "        auth = tweepy.OAuthHandler(consumerKey, consumerSecret)\n",
    "        auth.set_access_token(accessToken, accessTokenSecret)\n",
    "        api = tweepy.API(auth, wait_on_rate_limit = False)\n",
    "\n",
    "        # input for term to be searched and how many tweets to search\n",
    "        # searchTerm = input(\"Enter Keyword/Tag to search about: \")\n",
    "        # NoOfTerms = int(input(\"Enter how many tweets to search: \"))\n",
    "\n",
    "        # searching for tweets\n",
    "        self.tweets = tweepy.Cursor(api.search_tweets, q=searchTerm, lang = \"in\").items(NoOfTerms)\n",
    "\n",
    "        # Open/create a file to append data to\n",
    "        csvFile = open('result.csv', 'a')\n",
    "\n",
    "        # Use csv writer\n",
    "        csvWriter = csv.writer(csvFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # creating some variables to store info\n",
    "        polarity = 0\n",
    "        positive = 0\n",
    "        negative = 0\n",
    "        neutral = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (Temp/ipykernel_7480/3533646795.py, line 57)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\HP\\AppData\\Local\\Temp/ipykernel_7480/3533646795.py\"\u001b[1;36m, line \u001b[1;32m57\u001b[0m\n\u001b[1;33m    def load_stopwords(self):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "        # iterating through tweets fetched\n",
    "        for tweet in self.tweets:\n",
    "            #Append to temp so that we can store in csv later. I use encode UTF-8\n",
    "            self.tweetText.append(self.SVMmethod(tweet.text).encode('utf-8'))\n",
    "            # print (tweet.text.translate(non_bmp_map))    #print tweet's text\n",
    "            analysis = TextBlob(tweet.text)\n",
    "\n",
    "            # print(analysis.sentiment)  # print tweet's polarity\n",
    "            polarity += analysis.sentiment.polarity  # adding up polarities to find the average later\n",
    "            if (analysis.sentiment.polarity == 0):  # adding reaction of how people are reacting to find average later\n",
    "                neutral += 1\n",
    "            \n",
    "            elif (analysis.sentiment.polarity > 0 ):\n",
    "                positive += 1\n",
    "            \n",
    "            elif (analysis.sentiment.polarity < 0):\n",
    "                negative += 1\n",
    "            \n",
    "\n",
    "        # Write to csv and close csv file\n",
    "        csvWriter.writerow(self.tweetText)\n",
    "        csvFile.close()\n",
    "\n",
    "        # finding average of how people are reacting\n",
    "        positive = self.percentage(positive, NoOfTerms)\n",
    "       \n",
    "        negative = self.percentage(negative, NoOfTerms)\n",
    "        \n",
    "        neutral = self.percentage(neutral, NoOfTerms)\n",
    "\n",
    "        # finding average reaction\n",
    "        polarity = polarity / NoOfTerms\n",
    "\n",
    "        # printing out data\n",
    "        print(\"How people are reacting on \" + searchTerm + \" by analyzing \" + str(NoOfTerms) + \" tweets.\")\n",
    "        print()\n",
    "        print(\"General Report: \")\n",
    "        if (polarity == 0):\n",
    "            print(\"Neutral\")\n",
    "        \n",
    "        elif (polarity > 0 ):\n",
    "            print(\"Positive\")\n",
    "       \n",
    "        elif (polarity < 0):\n",
    "            print(\"Negative\")\n",
    "        \n",
    "        print()\n",
    "        print(\"Detailed Report: \")\n",
    "        print(str(positive) + \"% people thought it was positive\")\n",
    "        \n",
    "        print(str(negative) + \"% people thought it was negative\")\n",
    "        \n",
    "        print(str(neutral) + \"% people thought it was neutral\")\n",
    "        self.plotPieChart(positive, negative, neutral, searchTerm, NoOfTerms)\n",
    "        print('Accuracy score: ', accuracy_score(y_valid, y_pred))\n",
    "        self.SVMmethod(positive, negative, neutral)\n",
    "    def load_stopwords(self):\n",
    "        list_stopwords = stopwords.words('indonesian')\n",
    "        list_stopwords.extend(['yg', 'dg', 'dgn', 'ny', 'd', 'klo', \n",
    "                       'kalo', 'amp', 'biar', 'bikin', 'bilang', \n",
    "                       'gak', 'ga', 'krn', 'nya', 'nih', 'sih', \n",
    "                       'si', 'tau', 'tdk', 'tuh', 'utk', 'ya', \n",
    "                       'jd', 'jgn', 'sdh', 'aja', 'n', 't', \n",
    "                       'nyg', 'hehe', 'pen', 'u', 'nan', 'loh',\n",
    "                       '&amp', 'yah', 'klau', 'a', 'b', 'c', 'd', 'e', 'f',\n",
    "                       'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', \n",
    "                       's', 't', 'u', 'v', 'w', 'x', 'y', 'z'])\n",
    "        list_stopwords = set(list_stopwords)\n",
    "        return list_stopwords\n",
    "    def cleanTweet(self, tweet):\n",
    "        tweet = re.sub(r\"\\b[a-zA-Z]\\b\", \"\", tweet)\n",
    "        tweet = re.sub('[0-9]+', '', tweet)\n",
    "        tweet = tweet.replace('\\\\t',\" \").replace('\\\\n',\" \").replace('\\\\u',\" \").replace('\\\\',\"\")\n",
    "        tweet = tweet.encode('ascii', 'replace').decode('ascii')\n",
    "        tweet = re.sub(r'#', '', tweet)\n",
    "        tweet = tweet.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "        tweet = tweet.strip()\n",
    "        tweet = re.sub('\\s+',' ',tweet)\n",
    "        tweet = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t]) | (\\w +:\\ / \\ / \\S +)\", \" \", tweet).split())\n",
    "        tweet = word_tokenize(tweet)\n",
    "        if tweet not in stopwords.words('indonesian'):\n",
    "            return tweet\n",
    "        else:\n",
    "            return ''\n",
    "        tweet = [stemmer.stem(w) for w in tweet.split()]\n",
    "        return tweet\n",
    "    sencol =[]\n",
    "    senrow =np.array([])\n",
    "    nsen = 0\n",
    "    sentiment_list = []\n",
    "    def Lexiconmethod(self, tweet):\n",
    "        for i in range(len(df)):\n",
    "            nsen = senrow.shape[0]\n",
    "            words = word_tokenize(tweet)\n",
    "            sentiment = 0 \n",
    "            add = 0\n",
    "            prev = [0 for ii in range(len(words))]\n",
    "            n_words = len(words)\n",
    "            if len(sencol)>0:\n",
    "                sen =[0 for j in range(len(sencol))]\n",
    "            else:\n",
    "                sen =[]\n",
    "                for word in words:\n",
    "                    ind = words.index(word)\n",
    "                    if word in lexicon_word :\n",
    "                        sen,sencol,sentiment,add= found_word(ind,words,word,sen,sencol,sentiment,add)\n",
    "                    else:\n",
    "                        kata_dasar = stemmer.stem(word)\n",
    "                        if kata_dasar in lexicon_word:\n",
    "                            sen,sencol,sentiment,add= found_word(ind,words,kata_dasar,sen,sencol,sentiment,add)\n",
    "                        elif(n_words>1):\n",
    "                            if ind-1>-1:\n",
    "                                back_1    = words[ind-1]+' '+word\n",
    "                                if (back_1 in lexicon_word):\n",
    "                                    sen,sencol,sentiment,add= found_word(ind,words,back_1,sen,sencol,sentiment,add)\n",
    "                                elif(ind-2>-1):\n",
    "                                    back_2    = words[ind-2]+' '+back_1\n",
    "                                    if back_2 in lexicon_word:\n",
    "                                        sen,sencol,sentiment,add= found_word(ind,words,back_2,sen,sencol,sentiment,add)\n",
    "                                        if add>0:  \n",
    "                                            if i>0:\n",
    "                                                if (nsen==0):\n",
    "                                                    senrow = np.zeros([i,add],dtype=int)\n",
    "                                                elif(i!=nsen):\n",
    "                                                    padding_h = np.zeros([nsen,add],dtype=int)\n",
    "                                                    senrow = np.hstack((senrow,padding_h))\n",
    "                                                    padding_v = np.zeros([(i-nsen),senrow.shape[1]],dtype=int)\n",
    "                                                    senrow = np.vstack((senrow,padding_v))\n",
    "                                                else:\n",
    "                                                    padding =np.zeros([nsen,add],dtype=int)\n",
    "                                                    senrow = np.hstack((senrow,padding))\n",
    "                                                    senrow = np.vstack((senrow,sen))\n",
    "                                                    if i==0:\n",
    "                                                        senrow = np.array(sen).reshape(1,len(sen))\n",
    "                                                    elif(nsen>0):\n",
    "                                                        senrow = np.vstack((senrow,sen))\n",
    "                                                        sentiment_list.append(sentiment)\n",
    "    def SVMmethod(self,tweet):\n",
    "        train, valid = train_test_split(tweet, test_size=0.1, random_state=0)\n",
    "        tfidf_vectorizer=TfidfVectorizer()\n",
    "        tfidf_train=tfidf_vectorizer.fit_transform(X_train) \n",
    "        tfidf_test=tfidf_vectorizer.transform(X_test)\n",
    "        clf = svm.SVC(kernel='linear', probability=True)\n",
    "        clf.fit(tfidf_train, y_train)\n",
    "        y_pred = clf.fit(tfidf_train, y_train).predict(tfidf_test)\n",
    "        cm = metrics.confusion_matrix(y_valid, y_pred)\n",
    "\n",
    "    # function to calculate percentage\n",
    "    def percentage(self, part, whole):\n",
    "        temp = 100 * float(part) / float(whole)\n",
    "        return format(temp, '.2f')\n",
    "    def plotPieChart(self, positive, negative, neutral, searchTerm, noOfSearchTerms):\n",
    "        labels = ['Positive [' + str(positive) + '%]', 'Neutral [' + str(neutral) + '%]',\n",
    "                  'Negative [' + str(negative) + '%]']\n",
    "        sizes = [positive, neutral, negative]\n",
    "        colors = ['brown', 'lightcoral', 'beige']\n",
    "        patches, texts = plt.pie(sizes, colors=colors, startangle=90)\n",
    "        plt.legend(patches, labels, loc=\"best\")\n",
    "        plt.title('How people are reacting on \"' + searchTerm + '\" by analyzing ' + str(noOfSearchTerms) + ' Tweets.')\n",
    "        plt.axis('equal')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"static/fig.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if __name__== \"__main__\":<br>\n",
    "    sa = SentimentAnalysis()<br>\n",
    "    sa.DownloadData()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
