{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f787c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,tweepy,csv,re\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import nltk\n",
    "import Sastrawi\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, StopWordRemover, ArrayDictionary\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da6b53a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysis:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tweets = []\n",
    "        self.tweetText = []\n",
    "    def DownloadData(self,searchTerm, NoOfTerms):\n",
    "        # authenticating\n",
    "        print(searchTerm, NoOfTerms)\n",
    "        consumerKey='qubRP6d5B5eCD3RHrgoMRkDjH'\n",
    "        consumerSecret='AJlk1MiHfXqnhByWtCf1Mbi2T9WGW08hoLkkDIbks4ztUpFx99'\n",
    "        accessToken='3180202308-DmrgiSoxUUoSRevLLPldgCE57M0eD117K0mhFG5'\n",
    "        accessTokenSecret='fuY5Hkk838OnmGOZUb16KLkSDw3JZjflxXQ1NHP6pP9Jr'\n",
    "        auth = tweepy.OAuthHandler(consumerKey, consumerSecret)\n",
    "        auth.set_access_token(accessToken, accessTokenSecret)\n",
    "        api = tweepy.API(auth, wait_on_rate_limit = False)\n",
    "\n",
    "        # input for term to be searched and how many tweets to search\n",
    "        # searchTerm = input(\"Enter Keyword/Tag to search about: \")\n",
    "        # NoOfTerms = int(input(\"Enter how many tweets to search: \"))\n",
    "\n",
    "        # searching for tweets\n",
    "        self.tweets = tweepy.Cursor(api.search_tweets, q=searchTerm, lang = \"in\").items(NoOfTerms)\n",
    "        users = [[tweet.created_at, tweet.full_text] for tweet in posts]\n",
    "        df = pd.DataFrame(data=users, columns=['Created_At','Tweets'])\n",
    "        # Open/create a file to append data to\n",
    "        df.to_csv('hasilnya.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b81decbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "        polarity = 0\n",
    "        positive = 0\n",
    "        negative = 0\n",
    "        neutral = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7123c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # iterating through tweets fetched\n",
    "        for tweet in self.tweets:\n",
    "            #Append to temp so that we can store in csv later. I use encode UTF-8\n",
    "            self.tweetText.append(self.SVMmethod(tweet.text).encode('utf-8'))\n",
    "            # print (tweet.text.translate(non_bmp_map))    #print tweet's text\n",
    "            analysis = TextBlob(tweet.text)\n",
    "\n",
    "            # print(analysis.sentiment)  # print tweet's polarity\n",
    "            polarity += analysis.sentiment.polarity  # adding up polarities to find the average later\n",
    "\n",
    "            if (analysis.sentiment.polarity == 0):  # adding reaction of how people are reacting to find average later\n",
    "                neutral += 1\n",
    "            \n",
    "            elif (analysis.sentiment.polarity > 0 ):\n",
    "                positive += 1\n",
    "            \n",
    "            elif (analysis.sentiment.polarity < 0):\n",
    "                negative += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c86dafa9",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'hasilnya.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3348/2543643269.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtweet_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3348/2543643269.py\u001b[0m in \u001b[0;36mload_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'hasilnya.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtweet_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'hasilnya.csv'"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    data = pd.read_csv('hasilnya.csv')\n",
    "    return data\n",
    "\n",
    "tweet_df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4de100f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweet_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3348/1771754262.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Tweets'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#remove one char\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tweet_df' is not defined"
     ]
    }
   ],
   "source": [
    "df  = pd.DataFrame(tweet_df[['Tweets']])\n",
    "\n",
    "def remove(tweet):\n",
    "    \n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    #remove one char\n",
    "    tweet = re.sub(r\"\\b[a-zA-Z]\\b\", \"\", tweet)\n",
    "\n",
    "    #remove angka\n",
    "    tweet = re.sub('[0-9]+', '', tweet)\n",
    "    \n",
    "    # remove tab, new line, ans back slice\n",
    "    tweet = tweet.replace('\\\\t',\" \").replace('\\\\n',\" \").replace('\\\\u',\" \").replace('\\\\',\"\")\n",
    " \n",
    "    # remove non ASCII (emoticon, chinese word, .etc)\n",
    "    tweet = tweet.encode('ascii', 'replace').decode('ascii')\n",
    "    \n",
    "    # remove mention, link, hashtag\n",
    "    tweet = ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\",\" \", tweet).split())\n",
    "\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    \n",
    "    #remove punctuation\n",
    "    tweet = tweet.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    \n",
    "    #remove whitespace leading & trailing\n",
    "    tweet = tweet.strip()\n",
    "    \n",
    "    #remove multiple whitespace into single whitespace\n",
    "    tweet = re.sub('\\s+',' ',tweet)\n",
    "\n",
    "    return tweet\n",
    "\n",
    "df[\"Clean_Data\"] = df[\"Tweets\"].apply(remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16685953",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_word = pd.read_csv('Kamus_Baku.csv')\n",
    "\n",
    "normalized_word_dict = {}\n",
    "\n",
    "for index, row in normalized_word.iterrows():\n",
    "    if row[0] not in normalized_word_dict:\n",
    "        normalized_word_dict[row[0]] = row[1] \n",
    "\n",
    "def normalized_term(document):\n",
    "    return ' '.join([normalized_word_dict[term] if term in normalized_word_dict else term for term in document.split()])\n",
    "\n",
    "df['Tweet_Normal'] = df['Clean_Data'].apply(normalized_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65853e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=stopwords.words('indonesian')\n",
    "stop_words.extend([\"yg\", \"dg\", \"dgn\", \"ny\", \"d\", 'klo', \n",
    "                       'kalo', 'amp', 'biar', 'bikin', 'bilang', \n",
    "                       'gak', 'ga', 'krn', 'nya', 'nih', 'sih', \n",
    "                       'si', 'tau', 'tdk', 'tuh', 'utk', 'ya', \n",
    "                       'jd', 'jgn', 'sdh', 'aja', 'n', 't', \n",
    "                       'nyg', 'hehe', 'pen', 'u', 'nan', 'loh',\n",
    "                       '&amp', 'yah', 'klau', 'a', 'b', 'c', 'd', 'e', 'f',\n",
    "                   'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', \n",
    "                   's', 't', 'u', 'v', 'w', 'x', 'y', 'z'])\n",
    "\n",
    "df.columns = ['Tweets', 'Clean_Data', 'Tweet_Normal']\n",
    "\n",
    "df['Tanpa_StopWords'] = df['Tweet_Normal'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cdc020",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tweet_df[['Tweets', 'Clean_Data', 'Tweet_Normal', 'Tanpa_StopWords']])\n",
    "\n",
    "# create stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "df['Stemming'] = df['Tanpa_StopWords'].apply(lambda x : ([stemmer.stem(y) for y in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca59f4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sencol =[]\n",
    "senrow =np.array([])\n",
    "nsen = 0\n",
    "\n",
    "sentiment_list = []\n",
    "# function to write the word's sentiment if it is founded\n",
    "def found_word(ind,words,word,sen,sencol,sentiment,add):\n",
    "    # if it is already included in the bag of words matrix, then just increase the value\n",
    "    if word in sencol:\n",
    "        sen[sencol.index(word)] += 1\n",
    "    else:\n",
    "    #if not, than add new word\n",
    "        sencol.append(word)\n",
    "        sen.append(1)\n",
    "        add += 1\n",
    "    #if there is a negation word before it, the sentiment would be the negation of it's sentiment\n",
    "    if (words[ind-1] in negasi):\n",
    "        sentiment += -lexicon['weight'][lexicon_word.index(word)]\n",
    "    else:\n",
    "        sentiment += lexicon['weight'][lexicon_word.index(word)]\n",
    "    \n",
    "    return sen,sencol,sentiment,add\n",
    "            \n",
    "# checking every words, if they are appear in the lexicon, and then calculate their sentiment if they do\n",
    "for i in range(len(df)):\n",
    "    nsen = senrow.shape[0]\n",
    "    words = word_tokenize(df['Stemming'][i])\n",
    "    sentiment = 0 \n",
    "    add = 0\n",
    "    prev = [0 for ii in range(len(words))]\n",
    "    n_words = len(words)\n",
    "    if len(sencol)>0:\n",
    "        sen =[0 for j in range(len(sencol))]\n",
    "    else:\n",
    "        sen =[]\n",
    "    \n",
    "    for word in words:\n",
    "        ind = words.index(word)\n",
    "        # check whether they are included in the lexicon\n",
    "        if word in lexicon_word :\n",
    "            sen,sencol,sentiment,add= found_word(ind,words,word,sen,sencol,sentiment,add)\n",
    "        else:\n",
    "        # if not, then check the root word\n",
    "            kata_dasar = stemmer.stem(word)\n",
    "            if kata_dasar in lexicon_word:\n",
    "                sen,sencol,sentiment,add= found_word(ind,words,kata_dasar,sen,sencol,sentiment,add)\n",
    "        # if still negative, try to match the combination of words with the adjacent words\n",
    "            elif(n_words>1):\n",
    "                if ind-1>-1:\n",
    "                    back_1    = words[ind-1]+' '+word\n",
    "                    if (back_1 in lexicon_word):\n",
    "                        sen,sencol,sentiment,add= found_word(ind,words,back_1,sen,sencol,sentiment,add)\n",
    "                    elif(ind-2>-1):\n",
    "                        back_2    = words[ind-2]+' '+back_1\n",
    "                        if back_2 in lexicon_word:\n",
    "                            sen,sencol,sentiment,add= found_word(ind,words,back_2,sen,sencol,sentiment,add)\n",
    "    # if there is new word founded, then expand the matrix\n",
    "    if add>0:  \n",
    "        if i>0:\n",
    "            if (nsen==0):\n",
    "                senrow = np.zeros([i,add],dtype=int)\n",
    "            elif(i!=nsen):\n",
    "                padding_h = np.zeros([nsen,add],dtype=int)\n",
    "                senrow = np.hstack((senrow,padding_h))\n",
    "                padding_v = np.zeros([(i-nsen),senrow.shape[1]],dtype=int)\n",
    "                senrow = np.vstack((senrow,padding_v))\n",
    "            else:\n",
    "                padding =np.zeros([nsen,add],dtype=int)\n",
    "                senrow = np.hstack((senrow,padding))\n",
    "            senrow = np.vstack((senrow,sen))\n",
    "        if i==0:\n",
    "            senrow = np.array(sen).reshape(1,len(sen))\n",
    "    # if there isn't then just update the old matrix\n",
    "    elif(nsen>0):\n",
    "        senrow = np.vstack((senrow,sen))\n",
    "        \n",
    "    sentiment_list.append(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca6b619",
   "metadata": {},
   "outputs": [],
   "source": [
    "sencol.append('Bobot')\n",
    "sentiment_array = np.array(sentiment_list).reshape(senrow.shape[0],1)\n",
    "sentiment_data = np.hstack((senrow,sentiment_array))\n",
    "df_sen = pd.DataFrame(sentiment_data,columns = sencol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3317fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cek_df = pd.DataFrame([])\n",
    "cek_df['Tweetnya'] = df['Stemming'].copy()\n",
    "cek_df['Bobot']  = df_sen['Bobot'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee16e45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Bobot'] = df_sen['Bobot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a396c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['Bobot'] == 0, 'Label'] = 'netral'\n",
    "df.loc[df['Bobot'] > 0, 'Label'] = 'positif'\n",
    "df.loc[df['Bobot'] < 0, 'Label'] = 'negatif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc45836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[cek_df['Bobot'] == 0, 'Sentimen'] = 2 #netral\n",
    "df.loc[cek_df['Bobot'] > 0, 'Sentimen'] = 1 #positive\n",
    "df.loc[cek_df['Bobot'] < 0, 'Sentimen'] = 0 #negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557837ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(df['Stemming'], df['Sentimen'], test_size=0.1, random_state=0)\n",
    "\n",
    "train_df = pd.DataFrame()\n",
    "train_df['Stemming'] = x_train\n",
    "train_df['Sentimen'] = y_train\n",
    "test_df = pd.DataFrame()\n",
    "test_df['Stemming'] = x_test\n",
    "test_df['Sentimen'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0024c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()  \n",
    "\n",
    "docs = df['Stemming']\n",
    "\n",
    "x = tfidf.fit(docs)\n",
    "\n",
    "x_train_tfidf = x.transform(x_train)\n",
    "x_test_tfidf = x.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae99edb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "SVM = svm.SVC(kernel='linear')\n",
    "SVM.fit(x_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99011788",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_SVM = SVM.predict(x_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576aec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "confusion_matrix = confusion_matrix(y_test, predictions_SVM)\n",
    "\n",
    "print('Confusion Matrix : ') \n",
    "print(confusion_matrix)\n",
    "print('\\n Report Hasil : ') \n",
    "print(classification_report(y_test, predictions_SVM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3eb9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # function to calculate percentage\n",
    "    def percentage(self, part, whole):\n",
    "        temp = 100 * float(part) / float(whole)\n",
    "        return format(temp, '.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbb7103",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def plotPieChart(self, positive, negative, neutral, searchTerm, noOfSearchTerms):\n",
    "        labels = ['Positive [' + str(positive) + '%]', 'Neutral [' + str(neutral) + '%]',\n",
    "                  'Negative [' + str(negative) + '%]']\n",
    "        sizes = [positive, neutral, negative]\n",
    "        colors = ['brown', 'lightcoral', 'beige']\n",
    "        patches, texts = plt.pie(sizes, colors=colors, startangle=90)\n",
    "        plt.legend(patches, labels, loc=\"best\")\n",
    "        plt.title('How people are reacting on \"' + searchTerm + '\" by analyzing ' + str(noOfSearchTerms) + ' Tweets.')\n",
    "        plt.axis('equal')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"static/fig.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
