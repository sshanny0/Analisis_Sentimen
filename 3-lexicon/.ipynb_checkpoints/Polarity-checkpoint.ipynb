{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3c7d253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2515903b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Case_Folding</th>\n",
       "      <th>Data_Cleaning</th>\n",
       "      <th>without_stopwords</th>\n",
       "      <th>Tokenizing</th>\n",
       "      <th>tweet_normalized</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2 nasi padang lauk ayam sama sayur + es teh cu...</td>\n",
       "      <td>2 nasi padang lauk ayam sama sayur + es teh cu...</td>\n",
       "      <td>nasi padang lauk ayam sama sayur es teh cuman ...</td>\n",
       "      <td>nasi padang lauk ayam sayur es teh cuman k sho...</td>\n",
       "      <td>['nasi', 'padang', 'lauk', 'ayam', 'sayur', 'e...</td>\n",
       "      <td>nasi padang lauk ayam sayur es teh hanya k sho...</td>\n",
       "      <td>['nasi', 'padang', 'lauk', 'ayam', 'sayur', 'e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Mundur41149832 shopeefood klau ngakpunya shop...</td>\n",
       "      <td>@mundur41149832 shopeefood klau ngakpunya shop...</td>\n",
       "      <td>shopeefood klau ngakpunya shopepay gabisa dape...</td>\n",
       "      <td>shopeefood ngakpunya shopepay gabisa dapet vou...</td>\n",
       "      <td>['shopeefood', 'ngakpunya', 'shopepay', 'gabis...</td>\n",
       "      <td>shopeefood tidak punya shopepay tidak bisa dap...</td>\n",
       "      <td>['shopeefood', 'tidak', 'punya', 'shopepay', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spaghetti Aglio E Olio biasa? bukan! spaghetti...</td>\n",
       "      <td>spaghetti aglio e olio biasa? bukan! spaghetti...</td>\n",
       "      <td>spaghetti aglio e olio biasa bukan spaghetti m...</td>\n",
       "      <td>spaghetti aglio e olio spaghetti masakane simb...</td>\n",
       "      <td>['spaghetti', 'aglio', 'e', 'olio', 'spaghetti...</td>\n",
       "      <td>spaghetti aglio e olio spaghetti masakane simb...</td>\n",
       "      <td>['spaghetti', 'aglio', 'e', 'olio', 'spaghetti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ngakak + malu banget, masa pesen shopeefood ki...</td>\n",
       "      <td>ngakak + malu banget, masa pesen shopeefood ki...</td>\n",
       "      <td>ngakak malu banget masa pesen shopeefood kirim...</td>\n",
       "      <td>ngakak malu banget pesen shopeefood kirimnya t...</td>\n",
       "      <td>['ngakak', 'malu', 'banget', 'pesen', 'shopeef...</td>\n",
       "      <td>tertawa malu banget pesan shopeefood kirimnya ...</td>\n",
       "      <td>['tertawa', 'malu', 'banget', 'pesan', 'shopee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>barangkali ada yg tinggal di sekitar daerah pa...</td>\n",
       "      <td>barangkali ada yg tinggal di sekitar daerah pa...</td>\n",
       "      <td>barangkali ada yg tinggal di sekitar daerah pa...</td>\n",
       "      <td>barangkali tinggal daerah pasar rumput manggar...</td>\n",
       "      <td>['barangkali', 'tinggal', 'daerah', 'pasar', '...</td>\n",
       "      <td>barangkali tinggal daerah pasar rumput manggar...</td>\n",
       "      <td>['barangkali', 'tinggal', 'daerah', 'pasar', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Abis order shopeefood buat ortu trus mau order...</td>\n",
       "      <td>abis order shopeefood buat ortu trus mau order...</td>\n",
       "      <td>abis order shopeefood buat ortu trus mau order...</td>\n",
       "      <td>abis order shopeefood ortu trus order makan be...</td>\n",
       "      <td>['abis', 'order', 'shopeefood', 'ortu', 'trus'...</td>\n",
       "      <td>abis order shopeefood ortu trus order makan be...</td>\n",
       "      <td>['abis', 'order', 'shopeefood', 'ortu', 'trus'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>@kertas_angin @ShopeeID @shopeefood_id Trus ke...</td>\n",
       "      <td>@kertas_angin @shopeeid @shopeefood_id trus ke...</td>\n",
       "      <td>angin id trus kelanjutannya gimana kak aku bar...</td>\n",
       "      <td>angin id trus kelanjutannya gimana kak minggu ...</td>\n",
       "      <td>['angin', 'id', 'trus', 'kelanjutannya', 'gima...</td>\n",
       "      <td>angin id trus kelanjutannya gimana kak minggu ...</td>\n",
       "      <td>['angin', 'id', 'trus', 'lanjut', 'gimana', 'k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Emg ada ya bapak bapak namanya Emma? Heran ih ...</td>\n",
       "      <td>emg ada ya bapak bapak namanya emma? heran ih ...</td>\n",
       "      <td>emg ada ya bapak bapak namanya emma heran ih t...</td>\n",
       "      <td>emg namanya emma heran ih jajan shopeefood gue...</td>\n",
       "      <td>['emg', 'namanya', 'emma', 'heran', 'ih', 'jaj...</td>\n",
       "      <td>emg namanya emma heran ih jajan shopeefood say...</td>\n",
       "      <td>['emg', 'nama', 'emma', 'heran', 'ih', 'jajan'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Nasi Goreng Tiga De Nok - Palmerah sekarang ad...</td>\n",
       "      <td>nasi goreng tiga de nok - palmerah sekarang ad...</td>\n",
       "      <td>nasi goreng tiga de nok palmerah sekarang ada ...</td>\n",
       "      <td>nasi goreng de nok palmerah shopeefood pesan l...</td>\n",
       "      <td>['nasi', 'goreng', 'de', 'nok', 'palmerah', 's...</td>\n",
       "      <td>nasi goreng de nok palmerah shopeefood pesan l...</td>\n",
       "      <td>['nasi', 'goreng', 'de', 'nok', 'palmerah', 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>ajarin gimana cara make shopeefood</td>\n",
       "      <td>ajarin gimana cara make shopeefood</td>\n",
       "      <td>ajarin gimana cara make shopeefood</td>\n",
       "      <td>ajarin gimana make shopeefood</td>\n",
       "      <td>['ajarin', 'gimana', 'make', 'shopeefood']</td>\n",
       "      <td>ajarin gimana make shopeefood</td>\n",
       "      <td>['ajarin', 'gimana', 'make', 'shopeefood']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweets  \\\n",
       "0   2 nasi padang lauk ayam sama sayur + es teh cu...   \n",
       "1   @Mundur41149832 shopeefood klau ngakpunya shop...   \n",
       "2   Spaghetti Aglio E Olio biasa? bukan! spaghetti...   \n",
       "3   Ngakak + malu banget, masa pesen shopeefood ki...   \n",
       "4   barangkali ada yg tinggal di sekitar daerah pa...   \n",
       "..                                                ...   \n",
       "95  Abis order shopeefood buat ortu trus mau order...   \n",
       "96  @kertas_angin @ShopeeID @shopeefood_id Trus ke...   \n",
       "97  Emg ada ya bapak bapak namanya Emma? Heran ih ...   \n",
       "98  Nasi Goreng Tiga De Nok - Palmerah sekarang ad...   \n",
       "99                 ajarin gimana cara make shopeefood   \n",
       "\n",
       "                                         Case_Folding  \\\n",
       "0   2 nasi padang lauk ayam sama sayur + es teh cu...   \n",
       "1   @mundur41149832 shopeefood klau ngakpunya shop...   \n",
       "2   spaghetti aglio e olio biasa? bukan! spaghetti...   \n",
       "3   ngakak + malu banget, masa pesen shopeefood ki...   \n",
       "4   barangkali ada yg tinggal di sekitar daerah pa...   \n",
       "..                                                ...   \n",
       "95  abis order shopeefood buat ortu trus mau order...   \n",
       "96  @kertas_angin @shopeeid @shopeefood_id trus ke...   \n",
       "97  emg ada ya bapak bapak namanya emma? heran ih ...   \n",
       "98  nasi goreng tiga de nok - palmerah sekarang ad...   \n",
       "99                 ajarin gimana cara make shopeefood   \n",
       "\n",
       "                                        Data_Cleaning  \\\n",
       "0   nasi padang lauk ayam sama sayur es teh cuman ...   \n",
       "1   shopeefood klau ngakpunya shopepay gabisa dape...   \n",
       "2   spaghetti aglio e olio biasa bukan spaghetti m...   \n",
       "3   ngakak malu banget masa pesen shopeefood kirim...   \n",
       "4   barangkali ada yg tinggal di sekitar daerah pa...   \n",
       "..                                                ...   \n",
       "95  abis order shopeefood buat ortu trus mau order...   \n",
       "96  angin id trus kelanjutannya gimana kak aku bar...   \n",
       "97  emg ada ya bapak bapak namanya emma heran ih t...   \n",
       "98  nasi goreng tiga de nok palmerah sekarang ada ...   \n",
       "99                 ajarin gimana cara make shopeefood   \n",
       "\n",
       "                                    without_stopwords  \\\n",
       "0   nasi padang lauk ayam sayur es teh cuman k sho...   \n",
       "1   shopeefood ngakpunya shopepay gabisa dapet vou...   \n",
       "2   spaghetti aglio e olio spaghetti masakane simb...   \n",
       "3   ngakak malu banget pesen shopeefood kirimnya t...   \n",
       "4   barangkali tinggal daerah pasar rumput manggar...   \n",
       "..                                                ...   \n",
       "95  abis order shopeefood ortu trus order makan be...   \n",
       "96  angin id trus kelanjutannya gimana kak minggu ...   \n",
       "97  emg namanya emma heran ih jajan shopeefood gue...   \n",
       "98  nasi goreng de nok palmerah shopeefood pesan l...   \n",
       "99                      ajarin gimana make shopeefood   \n",
       "\n",
       "                                           Tokenizing  \\\n",
       "0   ['nasi', 'padang', 'lauk', 'ayam', 'sayur', 'e...   \n",
       "1   ['shopeefood', 'ngakpunya', 'shopepay', 'gabis...   \n",
       "2   ['spaghetti', 'aglio', 'e', 'olio', 'spaghetti...   \n",
       "3   ['ngakak', 'malu', 'banget', 'pesen', 'shopeef...   \n",
       "4   ['barangkali', 'tinggal', 'daerah', 'pasar', '...   \n",
       "..                                                ...   \n",
       "95  ['abis', 'order', 'shopeefood', 'ortu', 'trus'...   \n",
       "96  ['angin', 'id', 'trus', 'kelanjutannya', 'gima...   \n",
       "97  ['emg', 'namanya', 'emma', 'heran', 'ih', 'jaj...   \n",
       "98  ['nasi', 'goreng', 'de', 'nok', 'palmerah', 's...   \n",
       "99         ['ajarin', 'gimana', 'make', 'shopeefood']   \n",
       "\n",
       "                                     tweet_normalized  \\\n",
       "0   nasi padang lauk ayam sayur es teh hanya k sho...   \n",
       "1   shopeefood tidak punya shopepay tidak bisa dap...   \n",
       "2   spaghetti aglio e olio spaghetti masakane simb...   \n",
       "3   tertawa malu banget pesan shopeefood kirimnya ...   \n",
       "4   barangkali tinggal daerah pasar rumput manggar...   \n",
       "..                                                ...   \n",
       "95  abis order shopeefood ortu trus order makan be...   \n",
       "96  angin id trus kelanjutannya gimana kak minggu ...   \n",
       "97  emg namanya emma heran ih jajan shopeefood say...   \n",
       "98  nasi goreng de nok palmerah shopeefood pesan l...   \n",
       "99                      ajarin gimana make shopeefood   \n",
       "\n",
       "                                              stemmed  \n",
       "0   ['nasi', 'padang', 'lauk', 'ayam', 'sayur', 'e...  \n",
       "1   ['shopeefood', 'tidak', 'punya', 'shopepay', '...  \n",
       "2   ['spaghetti', 'aglio', 'e', 'olio', 'spaghetti...  \n",
       "3   ['tertawa', 'malu', 'banget', 'pesan', 'shopee...  \n",
       "4   ['barangkali', 'tinggal', 'daerah', 'pasar', '...  \n",
       "..                                                ...  \n",
       "95  ['abis', 'order', 'shopeefood', 'ortu', 'trus'...  \n",
       "96  ['angin', 'id', 'trus', 'lanjut', 'gimana', 'k...  \n",
       "97  ['emg', 'nama', 'emma', 'heran', 'ih', 'jajan'...  \n",
       "98  ['nasi', 'goreng', 'de', 'nok', 'palmerah', 's...  \n",
       "99         ['ajarin', 'gimana', 'make', 'shopeefood']  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Stemming.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e96a507b",
   "metadata": {},
   "outputs": [],
   "source": [
    "negasi = ['bukan','tidak','ga','gk']\n",
    "lexicon = pd.read_csv('full_lexicon.csv')\n",
    "lexicon = lexicon.drop(lexicon[(lexicon['word'] == 'bukan')\n",
    "                               |(lexicon['word'] == 'tidak')\n",
    "                               |(lexicon['word'] == 'ga')|(lexicon['word'] == 'gk') ].index,axis=0)\n",
    "lexicon = lexicon.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5217498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10284"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83e94374",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_word = lexicon['word'].to_list()\n",
    "lexicon_num_words = lexicon['number_of_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57aa38d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10284"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lexicon_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31d43e0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kata_dasar' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11984/1481999830.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0mkata_dasar\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlexicon_word\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m             \u001b[0mns_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mns_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'kata_dasar' is not defined"
     ]
    }
   ],
   "source": [
    "if kata_dasar not in lexicon_word:\n",
    "            ns_words.append(word)\n",
    "len(ns_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7192b8b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11984/1476924108.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Tweets'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Case_Folding'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Data_Cleaning'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'without_stopwords'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Tokenizing'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tweet_normalized'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'stemmed'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SWN analysis'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'stemmed'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentiwordnetanalysis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4355\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4356\u001b[0m         \"\"\"\n\u001b[1;32m-> 4357\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4359\u001b[0m     def _reduce(\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1041\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1043\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1044\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1097\u001b[0m                 \u001b[1;31m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 \u001b[1;31m# \"Callable[[Any], Any]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1099\u001b[1;33m                 mapped = lib.map_infer(\n\u001b[0m\u001b[0;32m   1100\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m                     \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11984/1476924108.py\u001b[0m in \u001b[0;36msentiwordnetanalysis\u001b[1;34m(pos_data)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0msentiment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mtokens_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpos_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "def load_data():\n",
    "    data = pd.read_csv('Stemming.csv')\n",
    "    return data\n",
    "\n",
    "tweet_df = load_data()\n",
    "tweet_df\n",
    "\n",
    "def sentiwordnetanalysis(pos_data):\n",
    "    sentiment = 0\n",
    "    tokens_count = 0\n",
    "    for word, pos in pos_data:\n",
    "        if not pos:\n",
    "            continue\n",
    "            lemma = wordnet_lemmatizer.lemmatize(word, pos=pos)\n",
    "        if not lemma:\n",
    "            continue\n",
    "            synsets = wordnet.synsets(lemma, pos=pos)\n",
    "        if not synsets:\n",
    "            continue\n",
    "            # Take the first sense, the most common\n",
    "            synset = synsets[0]\n",
    "            swn_synset = swn.senti_synset(synset.name())\n",
    "            sentiment += swn_synset.pos_score() - swn_synset.neg_score()\n",
    "            tokens_count += 1\n",
    "            # print(swn_synset.pos_score(),swn_synset.neg_score(),swn_synset.obj_score())\n",
    "        if not tokens_count:\n",
    "            return 0\n",
    "        if sentiment>0:\n",
    "            return \"Positive\"\n",
    "        if sentiment==0:\n",
    "            return \"Neutral\"\n",
    "        else:\n",
    "            return \"Negative\"\n",
    "\n",
    "df = pd.DataFrame(tweet_df[['Tweets', 'Case_Folding', 'Data_Cleaning', 'without_stopwords', 'Tokenizing', 'tweet_normalized', 'stemmed']])\n",
    "\n",
    "df['SWN analysis'] = df['stemmed'].apply(sentiwordnetanalysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e05f825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "df = pd.read_csv('Stemming.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "baa15d61",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_9088/1098156922.py, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\HP\\AppData\\Local\\Temp/ipykernel_9088/1098156922.py\"\u001b[1;36m, line \u001b[1;32m20\u001b[0m\n\u001b[1;33m    else:\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pos_list= open(\"kata_positif.txt\",\"r\")\n",
    "pos_kata = pos_list.readlines()\n",
    "neg_list= open(\"kata_negatif.txt\",\"r\")\n",
    "neg_kata = neg_list.readlines()  \n",
    "\n",
    "sentiments = [] #Membuat sebuah list menyimpan nilai sentiment\n",
    "\n",
    "#membuat list kata-kata negasi\n",
    "list_anti = ['tidak','lawan','anti', 'belum', 'belom', 'tdk', 'jangan', 'gak', 'enggak', 'bukan', 'sulit', 'tak', 'sblm']\n",
    "\n",
    "count_p = 0 #nilai positif\n",
    "count_n = 0 #nilai negatif\n",
    "    \n",
    "for tweet in df:\n",
    "    for kata_pos in pos_kata:\n",
    "        if kata_pos.strip().lower() == tweet.lower():\n",
    "            if items[items.index(item)-1] in list_anti:\n",
    "                print(items[items.index(item)-1], kata_pos, ['negatif'])\n",
    "                count_n += 1\n",
    "                else:\n",
    "                    print(kata_pos, ['positif'])\n",
    "                    count_p += 1\n",
    "                    for kata_neg in neg_kata:\n",
    "                        if kata_neg.strip().lower() == tweet.lower():\n",
    "                            if items[items.index(item)-1] in list_anti:\n",
    "                                print(items[items.index(item)-1], kata_neg, ['positif'])\n",
    "                                count_p += 1\n",
    "                                else:\n",
    "                                    print(kata_neg, ['negatif'])\n",
    "                                    count_n += 1\n",
    "                                    \n",
    "                                    print (\"positif: \"+str(count_p))\n",
    "                                    print (\"negatif: \"+str(count_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfac3059",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
